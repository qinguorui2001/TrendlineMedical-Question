{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "an0NQP5p6OqL",
        "outputId": "cc33aff6-45a9-4a26-e23b-ce91aa2f30d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0.1 0.1 0.1]\n",
            " [0.1 0.1 0.9]\n",
            " [0.1 0.9 0.1]\n",
            " [0.1 0.9 0.9]\n",
            " [0.9 0.1 0.1]\n",
            " [0.9 0.1 0.9]\n",
            " [0.9 0.9 0.1]\n",
            " [0.9 0.9 0.9]]\n",
            "[[0.1]\n",
            " [0.9]\n",
            " [0.9]\n",
            " [0.9]\n",
            " [0.9]\n",
            " [0.9]\n",
            " [0.9]\n",
            " [0.9]]\n",
            "[[ 0.01373626 -0.15796703 -0.15796703 -0.32967033  0.46703297  0.29532967\n",
            "   0.29532967  0.12362637]\n",
            " [ 0.01373626 -0.15796703  0.46703297  0.29532967 -0.15796703 -0.32967033\n",
            "   0.29532967  0.12362637]\n",
            " [ 0.01373626  0.46703297 -0.15796703  0.29532967 -0.15796703  0.29532967\n",
            "  -0.32967033  0.12362637]]\n",
            "[[0.48351648]\n",
            " [0.48351648]\n",
            " [0.48351648]]\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "\n",
        "#Assume inputs are human face photos with different angles looking at the phone, some of them with eye closed, some of them with eyes open. \n",
        "# Attach label 1 if eyes are open, 0 otherwise.\n",
        "#First step is to shuffle dataset using specific seed. Then split into training and validation datasets.\n",
        "# (if need to catch eye gaze, a page of current eyes' focus point needs to be given)\n",
        "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    \"/content/drive/My Drive/Colab Notebooks/dataset/\",\n",
        "    labels='inferred',\n",
        "    validation_split=0.2,\n",
        "    subset=\"training\",\n",
        "    seed=1337,\n",
        "    image_size=image_size,\n",
        "    batch_size=batch_size,\n",
        ")\n",
        "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    \"/content/drive/My Drive/Colab Notebooks/dataset/\",\n",
        "    labels='inferred',\n",
        "    validation_split=0.2,\n",
        "    subset=\"validation\",\n",
        "    seed=1337,\n",
        "    image_size=image_size,\n",
        "    batch_size=batch_size,\n",
        ")\n",
        "\n",
        "whole_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    \"/content/drive/My Drive/Colab Notebooks/dataset/\",\n",
        "    labels='inferred',\n",
        "    seed=1337,\n",
        "    image_size=image_size,\n",
        "    batch_size=batch_size,\n",
        ")\n",
        "#Second step is to do data augmentation, we do random flip and rotation and changing contrast of the image. \n",
        "# We don't do random corp since it adds complexity to detect eyes.\n",
        "data_augmentation = keras.Sequential(\n",
        "  [\n",
        "    layers.RandomFlip(\"horizontal\",\n",
        "                      input_shape=(img_height,\n",
        "                                  img_width,\n",
        "    #normalization is also possible                              3)),\n",
        "    layers.RandomRotation(0.1),\n",
        "  ]\n",
        ")\n",
        "# Then we extract features using convalutional layers, with maxpooling and batch normalization after each layer \n",
        "# to extract main features to reduce chance of overfitting.\n",
        "# If data size is large, underfitting still occurs, \n",
        "# at this point we may add extra convalutional layers to extract lower level detail features to better conclude the higher level details of an image\n",
        "model = Sequential([\n",
        "  layers.Rescaling(1./255, input_shape=(img_height, img_width, 3)),\n",
        "  layers.Conv2D(16, 3, padding='same', activation='relu'),\n",
        "  layers.MaxPooling2D(),\n",
        "  layers.batchNomalization(),\n",
        "  layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
        "  layers.MaxPooling2D(),\n",
        "  layers.batchNomalization(),\n",
        "  layers.batchNomalization(),\n",
        "  layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
        "  layers.MaxPooling2D(),\n",
        "  layers.batchNomalization(),\n",
        "  layers.Flatten(),\n",
        "  layers.Dense(128, activation='relu'),\n",
        "  layers.Dense(num_classes,activation='sigmoid')\n",
        "])\n",
        "# Then we check the validation set (once the validation accuracy rate not increase by 1% based on previous accuracy, just stop), \n",
        "# then we get the human eyes detection accuracy.\n",
        "\n",
        "# Further approach is use facial masks 2d matrix (low pass filter, high pass filter) to filter out the face in one image,\n",
        "# (non facial parts are eliminated) basically eyes are just located in range of faces\n",
        "# WE detect the range where 0(eliminated value) occurs in the range of face we detected before.\n",
        "# then we locate the eyes in real image, after extracting eyes, we can consider iris location easily due to different colors from surrounding Sclera. \n",
        "# After extracting, we just train on extracted iris with label(tired,\n",
        "# healthy, and so on) to achieve our main purpose. To catch eye gaze, we need to input pages of current eyes' focus point accordingly (as label) \n",
        "# and use regression model to reflect the (x,y) relationship. "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
